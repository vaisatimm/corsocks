Exam questions dump 

Question n° 1 - RuntimeClass

apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata: 
  name: untrusted 
handler: runsc

Create a pod with that runtimeclass. You must specify "runtimeClassName" inside the pod spec: 

apiVersion: v1 
kind: Pod 
metadata: 
  namespace: client 
spec: 
  runtimeClassName: 
  contaieners: 
    - name: foo 
      image: janedoe/awesomeapp:v1

Question n° 2

/cks/sa/pod1.yaml

apiVersion: v1
kind: Pod
metadata:
  name: backend
  namespace: qa
spec:
  serviceAccountName: dadong
  containers:
  - image: nginx:1.9
    imagePullPolicy: IfNotPresent
    name: backend

k create sa backend-sa -n qa --dry-run=client -o yaml > backend-sa.yaml

apiVersion: v1 
kind: ServiceAccount 
metadata: 
  name: backend-sa
  namespace: qa 

Set to not automatically mount API credentials 

apiVersion: v1 
kind: ServiceAccount 
metadata: 
  name: backend-sa
  namespace: qa 
automountServiceAccountToken: false 

k get sa -n qa 

k get po -n qa -o yaml | grep -i serviceaccount 
serviceAccountName: backend-sa
serviceAccountName: default 

k delete sa test01 -n qa

Question n° 3

Fix Kube-Apiserver:
Make a bck of kube-apiserver.yaml and run "kube-bench master"
- --authorization-mode=Node, RBAC  #set it like this
- --insecure-bind-address=0.0.0.0  #remove this 

Fix Kubelet: 
cp /var/lib/kubelet/config.yaml

apiVersion: ... 
authentication: 
  anonymous: 
    enabled: false #change true to false 
  webhook: 
    cacheTTL: 0s
    enabled: true 
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.cert
authorization: 
  mode: Webhook #Change to Webhook 
  webhook: 

Fix etcd: 

- --client-cert-auth=true #set to true 

systemctl daemon-reload 
systemctl restart kubelet 

Question n° 4 - Network Policy

The pod (products-service) in the namespace (dev-team) should be accessed only by: 
- pods in ns "qaqa";
- pods in any namespace with the label "environment:testing"

There is a skeleton of the manifest in the location set by the exercise: 


k get ns --show-labels
kubernetes.io/metadata.name=qa 

k get po -n dev-team --show-labels 
"environment:testing"

In case you don't have the labels you have to put them: 

k label ns qaqa kubernetes.io/metadata.name=qa 
k label pod products-service environment=testing -n dev-team 


netpol.yaml

apiVersion: networking.k8s.io/v1 
kind: NetworkPolicy 
metadata: 
  name: pod-restriction 
  namespace: dev-team 
spec: 
  podSelector: 
    matchLabels: 
      environment: testing 
  policyTypes: 
  - Ingress: 
  ingress: 
  - from: 
    - namespaceSelector: 
        matchLabels: 
          kubernetes.io/metadata.name: qa 
    - podSelector: 
        matchLabels:
          environment: testing 

Question n° 5 - TLS Version - CIPHER SUITES

Modify the kube-apiserver and insert: 

- --tls-cipher-suites=TLS_AES_128_GMC_SHA_256
- --tls-version=VersionTLS13

Modify the etcd.yaml and insert: 

- --cipher-suites=TLS_ECDE_RSA_WITH_AES_128_GMC_SHA256

Question n° 6 - default deny netpol

apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata: 
  name: denypolicy
  namespace: testing 
spec: 
  podSelector: {}
  policyTypes: 
  - Ingress 
  - Egress

Question n° 7 - Edit sa e create a new one with role and rolebinding

k edit role web-role -n db

resources: 
- services
verbs: 
- get

----- create a second role 

k create role role-2 --verb=delete --resources=namespaces -n db

k create rolebinding role-2-binding --role=role-2 --serviceaccount=db:service-account-web -n db 

Question n° 8 - Audit log Policy

Create: /etc/kubernetes/logpolicy/sample-policy.yaml 

apiVersion: audit.k8s.io/v1
kind: Policy 
omitStages:   
  - "RequestReceived"
rules:
  - level: RequestResponse
    resources: 
    - group: ""
      resources: ["persistentvolumes"]
  - level: Metadata 
    resources: 
    - group: "" 
      resources: ["secrets", "configmaps"]

  - level: Request
    resources: 
    - group: "" 
      resources: ["configmaps"]
    namespaces: ["front-apps"]

  - level: Metadata
    omitStages:
      - "RequestReceived"
!!! DON'T FORGET TO APPLY THE POLICY !!!

Turn on the audit loggin, modify: kube-apiserver

#Add some configuration 

- --audit-policy-life=/etc/kubernetes/logpolicy/sample-policy.yaml 
- --audit-log-path=/var/log/kubernetes/audit-log.txt
- --audit-log-maxage=10
- --audit-log-maxbackup=2

Check if the files are mounted, otherwise mount them: 

volumesMounts: 
- mountPath: /etc/kubernetes/logpolicy/sample-policy.yaml
  name: audit 
  readOnly: true 
- mountPath: /var/log/kubernetes/audit 
  name: audit-log 
  readOnly: true 

volumes: 
- hostPath: 
    path: /etc/kubernetes/logpolicy/sample-policy.yaml
    type: File 
  name: audit 
- hostPath:
    path: /var/log/kubernetes/
    type: DirectoryOrCreate

Check: tail -f /var/log/kubernetes/audit.log 
--------------------------------------------------------

Question n° 9 - extract secret data from a secret 

root@master01:~# kubectl get secrets -n istio-system db1-test -o yaml
apiVersion: v1
data:
  password: aGVsbG8=
  username: ZGlr
kind: Secret
metadata:
  creationTimestamp: "2023-05-21T14:58:44Z"
  name: db1-test
  namespace: istio-system
  resourceVersion: "116898"
  uid: bed8df2e-5c4d-4543-80a1-ba2392545510
type: Opaque

# decrypt username
echo -n 'ZGlr' | base64 -d > /cks/sec/user.txt

# decrypt password
echo -n 'aGVsbG8=' | base64 -d > /cks/sec/pass.txt

kubectl create secret generic db-test --from-literal=username=production-instance --from-literal=password=KvLfTkgs4VAH -n istio-system

apiVersion: v1 
kind: Pod 
metadata: 
  name: secret-pod 
  namespace: istio-system 
spec: 
  containers: 
  - image: nginx 
    name: dev-container
    volumeMounts: 
    - name: secret-volume 
      mountPath: /etc/secret 
  volumes: 
  - name: secret-volume 
    secret: 
      secretName: db2-test

Question n° 10 - Fix a dockerfile and a deployment 

###Dockerfile 
FROM ubuntu:16.04 #change it from latest to 16.04 

USER nobody #change it from root to nobody

USER nobody #change again user from root to nobody 

###Deployment

privileged: false (or False with capital F?)
readOnlyRootFilesystem: true (or True with capital T?)
runAsUser: 65535 (if it isn't present)

Question n° 11 - securityContext

edit a deploy and add under spec.containers (at the same level of volumeMounts): 

securityContext: 
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  runAsUser: 30000


For example: 

  template:
    metadata:
      creationTimestamp: null
      labels:
        app: security-context-deployment
    spec:
      containers:
      - image: nginx:stable
        imagePullPolicy: IfNotPresent
        name: nginx
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsUser: 3000

Remember that some security contexts directives can be set also at pod level so between spec and containers, like this: 

    spec: 
      securityContext: 
        ... 
      containers: 

But, the securityContext allowPrivilegeEscalation and readOnlyRootFilesystem can be set just at container level, like in the example before: 

    spec:
      containers:
      - image: nginx:stable
        imagePullPolicy: IfNotPresent
        name: nginx
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
          runAsUser: 3000


You can also make a mix: set the runAsUser at pod level and the others at containers level. Like here: 

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: security-context-deployment
  name: security-context-deployment
  namespace: securitycontext
spec:
  replicas: 1
  selector:
    matchLabels:
      app: security-context-deployment
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: security-context-deployment
    spec:
      securityContext:
        runAsUser: 3000
      containers:
      - image: nginx:stable
        name: nginx
        securityContext:
          allowPrivilegeEscalation: false
          readOnlyRootFilesystem: true
      dnsPolicy: ClusterFirst
      restartPolicy: Always

Remember that when you add the readOnlyRootFilesystem it could happen that the container goes in error state since it cannot write inside it's own directives to start the application: 

k logs security-context-deployment-d8fdbdd9f-bc84x
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: can not modify /etc/nginx/conf.d/default.conf (read-only file system?)
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2024/03/16 18:49:59 [warn] 1#1: the "user" directive makes sense only if the master process runs with super-user privileges, ignored in /etc/nginx/nginx.conf:2
nginx: [warn] the "user" directive makes sense only if the master process runs with super-user privileges, ignored in /etc/nginx/nginx.conf:2
2024/03/16 18:49:59 [emerg] 1#1: mkdir() "/var/cache/nginx/client_temp" failed (30: Read-only file system)
nginx: [emerg] mkdir() "/var/cache/nginx/client_temp" failed (30: Read-only file system)

--------------------------------------------------------

Question n° 12 - delete image with vulns 

ssh master01

k describe po -n kamino | grep -iE '^Name:|Image:'

trivy image -s HIGH,CRITICAL amazonlinux:1

k delete po [...]

_________________________________________________________________

Question n° 13 - Mode Node, RBAC and NodeRestriction

set: 

- --authorization-mode=Node,RBAC
- --enable-admission-plugins=NodeRestriction 
Finally: 
kubectl --kubeconfig=/etc/kubernetes/admin.conf delete clusterrolebinding system:anonymous 

Per chiarire ulteriormente le differenze tra --authorization-mode=Node e --enable-admission-plugins=NodeRestriction con un esempio, immaginiamo due scenari in un cluster Kubernetes:

Scenario 1: Creazione di un Pod
Con --authorization-mode=Node: Un kubelet tenta di creare un nuovo Pod su un nodo. Questo modo di autorizzazione verifica se il kubelet (e quindi il nodo da cui proviene la richiesta) ha l'autorizzazione per creare Pod. Se il nodo ha il permesso, basato sul controllo di autorizzazione, la richiesta procede. Questo passo assicura che solo i nodi autorizzati possano effettuare certe operazioni, come la creazione di Pod, limitando la possibilità che un nodo compromesso possa creare pod malevoli su altri nodi.

Con --enable-admission-plugins=NodeRestriction: Dopo che la richiesta è stata autorizzata, entra in gioco il plugin di ammissione NodeRestriction. Questo plugin impedisce al kubelet di creare Pod che non sono destinati al proprio nodo, oltre a restringere l'assegnazione di certi volumi o secret che non sono destinati per l'uso da parte di quei Pod. Questo fornisce un ulteriore livello di sicurezza che impedisce l'abuso di risorse a livello di cluster.

Scenario 2: Modifica di un Nodo
Con --authorization-mode=Node: Un kubelet tenta di aggiornare le informazioni di un oggetto Node, forse tentando di cambiare le etichette (labels) o le annotazioni. Il controllo di autorizzazione valuta se il nodo ha l'autorità per fare tale modifica. Questo modo di autorizzazione limita il nodo a effettuare solo modifiche autorizzate, impedendo, ad esempio, che un nodo compromesso possa etichettarsi in modo da ottenere privilegi o risorse non dovuti.

Con --enable-admission-plugins=NodeRestriction: Dopo che la richiesta di modifica è stata autorizzata, il plugin NodeRestriction applica regole specifiche che limitano le modifiche che un kubelet può fare sulle proprietà di un oggetto Node. Ad esempio, potrebbe impedire a un kubelet di modificare etichette che influenzerebbero la schedulazione dei Pod o l'assegnazione delle risorse, assicurando che le modifiche non compromettano la sicurezza o la stabilità operativa del cluster.

In entrambi gli scenari, l'uso combinato di --authorization-mode=Node e --enable-admission-plugins=NodeRestriction fornisce un robusto meccanismo di difesa in profondità. Il primo stabilisce se un nodo ha il diritto di fare una certa azione, mentre il secondo impone restrizioni dettagliate su ciò che può effettivamente fare dopo che la richiesta è stata approvata, garantendo che le operazioni sui nodi siano sia autorizzate che appropriate.


_________________________________________________________________

Question n° 14 - Apparmor 

ssh node02
sudo -iE
cd /etc/apparmor.d

cat nginx_apparmor | grep nginx-profile
nginx-profile-3

apparmor_parser /etc/apparmor.d/nginx_apparmor 

apparmor_status | grep nginx-profile-3

#Create a pod with the annotation: 

This is an example to undestand where to insert the annotation inside a deployment: 

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-apparmor-example
  labels:
    app: nginx-apparmor-example
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-apparmor-example
  template:
    metadata:
      labels:
        app: nginx-apparmor-example
      annotations:
        # Applica il profilo AppArmor 'docker-default' al container nginx
        container.apparmor.security.beta.kubernetes.io/nginx: localhost/docker-default
    spec:
      containers:
      - name: nginx
        image: nginx:stable
        ports:
        - containerPort: 80

Question n° 15 - Falco and Sysdig

sysdig -M 30 -p "%evt.time,%user.name,%proc.name" container.id=ahfoauso8o3qh8q >> details

sysdig -M 30 -p "%evt.time,%user.uid,%proc.name" container.id=ahfoauso8o3qh8q >> details 

________________________________________________________________________________

Question n° 16 - Setting the ImagePolicyWebhook 

Incomplete configuration in: 

/etc/kubernetes/epconfig

Container image scanner: 

https://image-bouncer-webhook.default.svc:1323/image_policy

1 - Enable the plugin to create the mirror policy: 
2 - Change the control configuration and set it to implicit deny: 
3 - Edit the configuration to correctly point to the provided https endpoint: 


Finally attempt to create a deploy with a vurnerable resource whose manifest is at /cks/img/web1.yaml

/etc/kubernetes/epconfig/admission_configuration.json 

...
  defaultAllow: false
... 

/etc/kubernetes/epconfig/kubeconfig.yml

...
  certificate-authority: /etc/kubernetes/epconfig/server.crt 
  server: https://image-bouncer-webhook.default.svc:1323/image_policy  #Add webhook server address 
  name: bouncer_webhook 

/etc/kubernetes/kube-apiserver.yaml 

- --enable-admission-plugins=NodeRestriction,ImagePolicyWebhook 
- --admission-control-config-file=/etc/kubernetes/epconfig/admission_configuration.json 
